{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99e10973",
   "metadata": {},
   "source": [
    "# Google Search Scraper Program\n",
    "\n",
    "* To run this program, either click \"Run\" in order on the code cells below, or go to Cell > Run All.\n",
    "\n",
    "* This program will output a list of links for a given Google search query, and save them to the file: (search_query)_search_results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ed3c02",
   "metadata": {},
   "source": [
    "# Other notes:\n",
    "\n",
    "1. This program asks the user to provide a search query and number of results they would like, or has the option of running with the default values: 1 page of results (10 results), and the search query \"mapuche tribe argentina chile.\"\n",
    "\n",
    "2. This program puts your search request to Google directly, so it is able to refine searches using the same operators that exist in the Google engine.\n",
    " * This includes specifying only a certain domain (ex: site:wikipedia.org), performing AND or OR queries, and searching for an exact phrase with quotation marks (\"exact phrase here\").  \n",
    " * See this page for reference (https://support.google.com/websearch/answer/2466433?hl=en).\n",
    "\n",
    "3. Google may limit requests after running this program a certain amount of times because it will think you are a bot. If you run into issues fetching results, it is likely you will need to change the \"user_agent\" variable in the 4th cell.\n",
    "\n",
    "4. You will need to have installed the Jupyter Notebook, requests, and BeautifulSoup libraries, if you haven't already. If you are running with Anaconda, they should already be installed. If not, the commands for doing so are as follows, which you will need to type in the terminal (Command line for Windows).\n",
    "        * python -m pip install notebook\n",
    "        * python -m pip install requests\n",
    "        * python -m pip install beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34529e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "789bf134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: You may need to change user_agent to your own user agent, given by this page: \n",
    "# https://www.whatismybrowser.com/detect/what-is-my-user-agent\n",
    "# (see this StackOverflow answer: https://stackoverflow.com/a/66866462)\n",
    "user_agent = None \n",
    "\n",
    "# sample user agent\n",
    "# user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36\"\" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "024698d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_anchor_url(url):\n",
    "    url = url.replace('/url?q=', '')\n",
    "    url = url.split('&sa=', 1)[0]\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_page_url_list(num, query):\n",
    "    search_engine_prefix = \"https://google.com/search?q=\"\n",
    "    urls = []\n",
    "    for i in range(num):\n",
    "        urls.append(search_engine_prefix + query + \"&start=\" + str((i) * 10))\n",
    "    return urls\n",
    "\n",
    "\n",
    "def get_results_on_page(url, output_file, counter):\n",
    "#     user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36\"\" \"\n",
    "    headers = {\n",
    "        \"User-Agent\": user_agent}\n",
    "    try:\n",
    "        result = requests.get(url, headers=headers)\n",
    "    except (requests.ConnectionError, requests.Timeout):\n",
    "        print(\"Issue connecting to website. Check URL and internet connection.\")\n",
    "        return False, counter\n",
    "    soup = BeautifulSoup(result.text, \"html.parser\")\n",
    "\n",
    "    h3s = soup.find_all('h3')\n",
    "    \n",
    "    link_strip_regex = re.compile(\"http.*&sa=U\")\n",
    "    \n",
    "    output_file.write(\"url,\" + url + '\\n')\n",
    "    print(\"\\nurl,\", url)\n",
    "    \n",
    "    for h in h3s:\n",
    "        url_title = h.getText()\n",
    "        try:\n",
    "            link = h.parent\n",
    "            if link.name == 'a':\n",
    "                link_url = link.get('href')\n",
    "                link_url = strip_anchor_url(link_url)\n",
    "                output_file.write(str(counter) + \",\" + url_title + ',' + link_url + '\\n')\n",
    "                print(str(counter), url_title, link_url, sep=',')\n",
    "                counter += 1\n",
    "        except AttributeError:\n",
    "            continue\n",
    "    return True, counter\n",
    "\n",
    "\n",
    "def get_inputs():\n",
    "    default_selection = input(\"Use default search settings (yes/no) ? (type q or quit to quit) \").lower()\n",
    "    if default_selection == \"yes\" or default_selection == 'y':\n",
    "        text = \"mapuche tribe argentina chile\"\n",
    "        num_results = 1\n",
    "    elif default_selection == \"q\" or default_selection =='quit':\n",
    "        return None, None\n",
    "    else:\n",
    "        text = input(\"Enter search query: \")\n",
    "        pages_of_results = input(\"Enter how many pages of results you would like (1 page = ~10 results): \")\n",
    "        try:\n",
    "            num_results = int(pages_of_results)\n",
    "            if num_results > 10:\n",
    "                num_results = 10\n",
    "        except ValueError:\n",
    "            print(\"Invalid input, defaulting to 1.\")\n",
    "            num_results = 1\n",
    "\n",
    "    return text, num_results\n",
    "\n",
    "\n",
    "def main():\n",
    "    text, pages_of_results = get_inputs()\n",
    "    counter = 1\n",
    "    while text != None:\n",
    "        file_name = text.replace(' ', '_') + \"_search_results.txt\"\n",
    "        output_file = open(file_name, \"w\", encoding='utf-8') \n",
    "        text = text.replace(' ', '+')\n",
    "        search_engine_prefix = \"https://google.com/search?q=\"\n",
    "        page_urls = get_page_url_list(pages_of_results, text)\n",
    "        for search_url in page_urls:\n",
    "            results_exist, ctr = get_results_on_page(search_url, output_file, counter)\n",
    "            if not results_exist:\n",
    "                break\n",
    "            else:\n",
    "                counter = ctr\n",
    "        counter = 1\n",
    "        output_file.close()\n",
    "        print(\"Query finished.\\n\")\n",
    "        text, pages_of_results = get_inputs()\n",
    "        \n",
    "    print(\"Program finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f4b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67298360",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
